{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmsdNZgkMZLy1BAIciZnuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CryptoVizArt/kiboMoney_riskDashboard_streamlit/blob/main/kibo_sample_signalExtraction_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHLeNaXstxYY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_single_data(url, start_date=\"2023-01-01\", end_date=\"2025-03-14\"):\n",
        "    start_year = int(start_date.split('-')[0])\n",
        "    end_year = int(end_date.split('-')[0])\n",
        "    years = list(range(start_year, end_year + 1))\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for year in years:\n",
        "        api_url = f\"{url}?chunk={year}\"\n",
        "        try:\n",
        "            response = requests.get(api_url)\n",
        "            data = response.json()\n",
        "            if 'dataset' in data and 'map' in data['dataset']:\n",
        "                all_data.update(data['dataset']['map'])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if all_data:\n",
        "        df = pd.DataFrame({\n",
        "            'date': list(all_data.keys()),\n",
        "            'value': list(all_data.values())\n",
        "        })\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df = df[(df['date'] >= pd.to_datetime(start_date)) &\n",
        "                (df['date'] <= pd.to_datetime(end_date))]\n",
        "        df = df.sort_values('date').reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    return pd.DataFrame(columns=['date', 'value'])\n",
        "\n",
        "def fetch_all_metrics(metrics_urls, start_date=\"2023-01-01\", end_date=\"2025-03-14\"):\n",
        "    # Dictionary to hold all dataframes\n",
        "    metric_dfs = {}\n",
        "\n",
        "    for url in metrics_urls:\n",
        "        metric_name = url.split('/')[-1]\n",
        "        df = fetch_single_data(url, start_date, end_date)\n",
        "\n",
        "        if not df.empty:\n",
        "            # Store dataframe with metric name as key\n",
        "            metric_dfs[metric_name] = df.set_index('date')['value']\n",
        "\n",
        "    if metric_dfs:\n",
        "        # Combine all series side by side with date as index\n",
        "        raw_data = pd.DataFrame(metric_dfs)\n",
        "        return raw_data\n",
        "\n",
        "    return pd.DataFrame()\n",
        "\n",
        "# Example usage:\n",
        "METRICS = [\n",
        "    \"https://kibo.money/api/date-to-realized-cap\",\n",
        "    \"https://kibo.money/api/date-to-supply-in-profit-to-circulating-supply-ratio\",\n",
        "    \"https://kibo.money/api/date-to-market-cap\",\n",
        "    \"https://kibo.money/api/date-to-realized-profit-to-loss-1d-sum-ratio\"\n",
        "]\n",
        "\n",
        "start_date = \"2021-01-01\"\n",
        "end_date = \"2025-03-14\"\n",
        "\n",
        "raw_data = fetch_all_metrics(METRICS, start_date, end_date)\n",
        "print(f\"Retrieved data for {raw_data.shape[1]} metrics across {raw_data.shape[0]} dates\")\n",
        "print(raw_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "from scipy import stats\n",
        "from scipy import signal\n",
        "from statsmodels.tsa.filters.hp_filter import hpfilter\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def extract_signals(raw_data, signal_config):\n",
        "    \"\"\"\n",
        "    Extract signals from a data frame using specified calculations, smoothing, and normalization methods.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_data : pandas.DataFrame\n",
        "        Input data frame containing the raw data\n",
        "    signal_config : dict\n",
        "        Dictionary with keys as column indices/names and values as lists containing:\n",
        "        [indicator_formula, smoothing_method, smoothing_window, normalization_method, normalization_window]\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Data frame containing the extracted signals\n",
        "\n",
        "    Available Smoothing Methods:\n",
        "    ----------------------------\n",
        "    - Moving Averages:\n",
        "        'sma': Simple Moving Average\n",
        "        'wma': Weighted Moving Average\n",
        "        'ema': Exponential Moving Average\n",
        "        'hma': Hull Moving Average\n",
        "    - Regression-Based Methods:\n",
        "        'linear_regression': Linear Regression Smoothing\n",
        "        'loess'/'lowess': Locally Weighted Scatterplot Smoothing\n",
        "        'polynomial': Polynomial Regression\n",
        "    - Filter-Based Methods:\n",
        "        'kalman': Kalman Filters\n",
        "        'hodrick_prescott'/'hp': Hodrick-Prescott Filter\n",
        "    - Statistical Methods:\n",
        "        'arima': ARIMA Models\n",
        "        'kernel': Kernel Smoothing\n",
        "\n",
        "    Available Normalization Methods:\n",
        "    -------------------------------\n",
        "    - Statistical Normalization:\n",
        "        'z_score'/'zscore': Standard Score (Z-Score)\n",
        "        'modified_z_score': Modified Z-Score using median and MAD\n",
        "        'min_max'/'minmax': Min-Max Scaling to [0,1]\n",
        "        'robust_scaling': Uses 5% and 95% quantiles instead of min-max\n",
        "        'deviation_from_mean': Simple subtraction of mean\n",
        "    - Time Series Normalization:\n",
        "        'differencing': First-order differences\n",
        "        'percentage_change': Relative changes in percentage\n",
        "        'log_transform': Natural logarithm transformation\n",
        "        'box_cox': Box-Cox power transformation\n",
        "        'detrending': Linear detrending\n",
        "        'rolling_normalization': Z-score within rolling windows\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    >>> # Create sample DataFrame\n",
        "    >>> raw_data = pd.DataFrame({\n",
        "    ...     'price': [100, 102, 99, 101, 103, 102, 104, 105, 103, 106],\n",
        "    ...     'volume': [1000, 1200, 950, 1100, 1300, 1250, 1400, 1500, 1350, 1600]\n",
        "    ... })\n",
        "    >>>\n",
        "    >>> # Define signal configuration\n",
        "    >>> signal_config = {\n",
        "    ...     'price': ['column', 'ema', 5, 'z_score', 10],  # Use column directly with EMA smoothing and Z-score normalization\n",
        "    ...     'volume': ['np.log(column)', 'sma', 3, 'min_max', 5],  # Apply log transform before smoothing\n",
        "    ...     ('price', 'volume'): ['raw_data[\"price\"] / raw_data[\"volume\"]', 'kalman', 4, 'percentage_change', None]  # Price/volume ratio\n",
        "    ... }\n",
        "    >>>\n",
        "    >>> # Extract signals\n",
        "    >>> signals = extract_signals(raw_data, signal_config)\n",
        "    >>> print(signals)\n",
        "    \"\"\"\n",
        "    result_df = pd.DataFrame(index=raw_data.index)\n",
        "\n",
        "    for col_key, config in signal_config.items():\n",
        "        # Parse configuration\n",
        "        indicator_formula, smoothing_method, smoothing_window, normalization_method, normalization_window = config\n",
        "\n",
        "        # Step 1: Calculate the indicator using the formula\n",
        "        if isinstance(col_key, (list, tuple)):\n",
        "            # If multiple columns are specified, extract them\n",
        "            required_cols = raw_data[list(col_key)]\n",
        "            # Evaluate the formula in the context of the required columns\n",
        "            # This allows formulas like \"col1 / col2\" or more complex expressions\n",
        "            indicator_series = eval(indicator_formula, {\"__builtins__\": {}},\n",
        "                                    {col: required_cols[col] for col in required_cols.columns})\n",
        "        else:\n",
        "            # If a single column is specified\n",
        "            indicator_series = eval(indicator_formula.replace(\"column\", f\"raw_data['{col_key}']\"),\n",
        "                                   {\"__builtins__\": {}, \"raw_data\": raw_data, \"np\": np})\n",
        "\n",
        "        # Step 2: Apply smoothing method\n",
        "        smoothed_series = apply_smoothing(indicator_series, smoothing_method, smoothing_window)\n",
        "\n",
        "        # Step 3: Apply normalization method\n",
        "        normalized_series = apply_normalization(smoothed_series, normalization_method, normalization_window)\n",
        "\n",
        "        # Add to result dataframe\n",
        "        signal_name = f\"{col_key}_signal\"\n",
        "        if isinstance(col_key, (list, tuple)):\n",
        "            signal_name = f\"{'_'.join(col_key)}_signal\"\n",
        "        result_df[signal_name] = normalized_series\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def apply_smoothing(series, method, window):\n",
        "    \"\"\"Apply the specified smoothing method to the series\"\"\"\n",
        "    if pd.isna(method) or method is None or method.lower() == 'none':\n",
        "        return series\n",
        "\n",
        "    # Ensure window is an integer for methods that require it\n",
        "    if window is not None and not pd.isna(window):\n",
        "        window = int(window)\n",
        "\n",
        "    # Moving Average Methods\n",
        "    if method.lower() == 'sma':\n",
        "        return series.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "    elif method.lower() == 'wma':\n",
        "        weights = np.arange(1, window + 1)\n",
        "        return series.rolling(window=window, min_periods=1).apply(\n",
        "            lambda x: np.sum(weights * x) / np.sum(weights[:len(x)]), raw=True)\n",
        "\n",
        "    elif method.lower() == 'ema':\n",
        "        return series.ewm(span=window, min_periods=1, adjust=False).mean()\n",
        "\n",
        "    elif method.lower() == 'hma':\n",
        "        # Hull Moving Average: WMA(2*WMA(n/2) - WMA(n)), sqrt(n)\n",
        "        half_window = max(window // 2, 1)\n",
        "        wma_n = series.rolling(window=window, min_periods=1).apply(\n",
        "            lambda x: np.sum(np.arange(1, len(x) + 1) * x) / np.sum(np.arange(1, len(x) + 1)), raw=True)\n",
        "        wma_half = series.rolling(window=half_window, min_periods=1).apply(\n",
        "            lambda x: np.sum(np.arange(1, len(x) + 1) * x) / np.sum(np.arange(1, len(x) + 1)), raw=True)\n",
        "        hull_series = 2 * wma_half - wma_n\n",
        "        sqrt_window = max(int(np.sqrt(window)), 1)\n",
        "        return hull_series.rolling(window=sqrt_window, min_periods=1).apply(\n",
        "            lambda x: np.sum(np.arange(1, len(x) + 1) * x) / np.sum(np.arange(1, len(x) + 1)), raw=True)\n",
        "\n",
        "    # Regression-Based Methods\n",
        "    elif method.lower() == 'linear_regression':\n",
        "        smoothed = series.copy()\n",
        "        for i in range(len(series) - window + 1):\n",
        "            y = series.iloc[i:i+window].values\n",
        "            x = np.arange(len(y))\n",
        "            slope, intercept, _, _, _ = stats.linregress(x, y)\n",
        "            smoothed.iloc[i+window-1] = intercept + slope * (window - 1)\n",
        "        # Fill NaN values at the beginning\n",
        "        if window > 1:\n",
        "            for i in range(window - 1):\n",
        "                y = series.iloc[:i+1].values\n",
        "                if len(y) > 1:  # Need at least 2 points for regression\n",
        "                    x = np.arange(len(y))\n",
        "                    slope, intercept, _, _, _ = stats.linregress(x, y)\n",
        "                    smoothed.iloc[i] = intercept + slope * i\n",
        "                else:\n",
        "                    smoothed.iloc[i] = series.iloc[i]\n",
        "        return smoothed\n",
        "\n",
        "    elif method.lower() in ['loess', 'lowess']:\n",
        "        # LOWESS smoothing\n",
        "        # For a more accurate implementation, convert window to fraction\n",
        "        fraction = min(max(window / len(series), 0.05), 1.0)\n",
        "        result = lowess(series.values, np.arange(len(series)), frac=fraction, return_sorted=False)\n",
        "        return pd.Series(result, index=series.index)\n",
        "\n",
        "    elif method.lower() == 'polynomial':\n",
        "        # Polynomial regression smoothing\n",
        "        degree = min(3, window - 1)  # Degree of polynomial, default to cubic or less\n",
        "        smoothed = series.copy()\n",
        "        x = np.arange(len(series))\n",
        "\n",
        "        # Fit polynomial to entire series\n",
        "        coeffs = np.polyfit(x, series.values, degree)\n",
        "        smoothed_values = np.polyval(coeffs, x)\n",
        "        return pd.Series(smoothed_values, index=series.index)\n",
        "\n",
        "    # Filter-Based Methods\n",
        "    elif method.lower() == 'kalman':\n",
        "        # Simple Kalman filter implementation\n",
        "        smoothed = series.copy()\n",
        "        n = len(series)\n",
        "        # State transition model\n",
        "        A = 1\n",
        "        # Observation model\n",
        "        H = 1\n",
        "        # Process noise\n",
        "        Q = 0.01\n",
        "        # Observation noise (higher for more smoothing)\n",
        "        R = max(0.1, 1.0 / window)\n",
        "\n",
        "        # Initial state estimate\n",
        "        x_hat = series.iloc[0]\n",
        "        # Initial error covariance\n",
        "        P = 1.0\n",
        "\n",
        "        for i in range(n):\n",
        "            # Predict\n",
        "            x_hat_minus = A * x_hat\n",
        "            P_minus = A * P * A + Q\n",
        "\n",
        "            # Update\n",
        "            K = P_minus * H / (H * P_minus * H + R)\n",
        "            x_hat = x_hat_minus + K * (series.iloc[i] - H * x_hat_minus)\n",
        "            P = (1 - K * H) * P_minus\n",
        "\n",
        "            smoothed.iloc[i] = x_hat\n",
        "\n",
        "        return smoothed\n",
        "\n",
        "    elif method.lower() == 'hodrick_prescott' or method.lower() == 'hp':\n",
        "        # Hodrick-Prescott filter\n",
        "        # Lambda parameter based on window size (higher lambda = smoother trend)\n",
        "        lambda_param = max(1600, window * 100)\n",
        "        cycle, trend = hpfilter(series, lamb=lambda_param)\n",
        "        return trend\n",
        "\n",
        "    # Statistical Methods\n",
        "    elif method.lower() == 'arima':\n",
        "        # Simple ARIMA implementation\n",
        "        # Using auto_arima would be better but requires pmdarima\n",
        "        try:\n",
        "            # Using a simple (1,1,1) ARIMA model\n",
        "            model = sm.tsa.ARIMA(series, order=(1, 1, 1))\n",
        "            results = model.fit()\n",
        "            return results.fittedvalues\n",
        "        except:\n",
        "            # Fallback to EMA if ARIMA fails\n",
        "            return series.ewm(span=window, min_periods=1, adjust=False).mean()\n",
        "\n",
        "    elif method.lower() == 'kernel':\n",
        "        # Gaussian kernel smoothing\n",
        "        window_vals = np.arange(-window//2, window//2 + 1)\n",
        "        kernel = np.exp(-(window_vals**2) / (2 * (window/4)**2))\n",
        "        kernel = kernel / np.sum(kernel)\n",
        "\n",
        "        # Apply convolution for smoothing\n",
        "        smoothed_values = np.convolve(series.fillna(method='ffill').fillna(method='bfill').values,\n",
        "                                      kernel, mode='same')\n",
        "        return pd.Series(smoothed_values, index=series.index)\n",
        "\n",
        "    # If method not recognized, return original series\n",
        "    return series\n",
        "\n",
        "def apply_normalization(series, method, window):\n",
        "    \"\"\"Apply the specified normalization method to the series\"\"\"\n",
        "    if pd.isna(method) or method is None or method.lower() == 'none':\n",
        "        return series\n",
        "\n",
        "    # Ensure window is an integer for methods that require it\n",
        "    if window is not None and not pd.isna(window):\n",
        "        window = int(window)\n",
        "    else:\n",
        "        window = len(series)  # Default to full series length\n",
        "\n",
        "    # Statistical Normalization Methods\n",
        "    if method.lower() == 'z_score' or method.lower() == 'zscore':\n",
        "        if window == len(series):\n",
        "            # Global Z-score\n",
        "            return (series - series.mean()) / series.std(ddof=0)\n",
        "        else:\n",
        "            # Rolling Z-score\n",
        "            means = series.rolling(window=window, min_periods=1).mean()\n",
        "            stds = series.rolling(window=window, min_periods=1).std(ddof=0)\n",
        "            # Prevent division by zero\n",
        "            stds = stds.replace(0, np.nan).fillna(1)\n",
        "            return (series - means) / stds\n",
        "\n",
        "    elif method.lower() == 'modified_z_score':\n",
        "        if window == len(series):\n",
        "            # Global modified Z-score\n",
        "            median = series.median()\n",
        "            mad = (series - median).abs().median()\n",
        "            mad = mad if mad != 0 else 1  # Prevent division by zero\n",
        "            return 0.6745 * (series - median) / mad\n",
        "        else:\n",
        "            # Rolling modified Z-score\n",
        "            medians = series.rolling(window=window, min_periods=1).median()\n",
        "            # Calculate rolling MAD\n",
        "            def rolling_mad(x):\n",
        "                return (x - x.median()).abs().median()\n",
        "            mads = series.rolling(window=window, min_periods=1).apply(\n",
        "                rolling_mad, raw=False)\n",
        "            # Prevent division by zero\n",
        "            mads = mads.replace(0, np.nan).fillna(1)\n",
        "            return 0.6745 * (series - medians) / mads\n",
        "\n",
        "    elif method.lower() == 'min_max' or method.lower() == 'minmax':\n",
        "        if window == len(series):\n",
        "            # Global min-max scaling\n",
        "            min_val = series.min()\n",
        "            max_val = series.max()\n",
        "            if min_val == max_val:\n",
        "                return pd.Series(0.5, index=series.index)  # All values are the same\n",
        "            return (series - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            # Rolling min-max scaling\n",
        "            mins = series.rolling(window=window, min_periods=1).min()\n",
        "            maxs = series.rolling(window=window, min_periods=1).max()\n",
        "            # Handle case where min equals max\n",
        "            denom = (maxs - mins).replace(0, np.nan).fillna(1)\n",
        "            return (series - mins) / denom\n",
        "\n",
        "    elif method.lower() == 'robust_scaling':\n",
        "        if window == len(series):\n",
        "            # Global robust scaling using 5% and 95% quantiles\n",
        "            q1 = series.quantile(0.05)\n",
        "            q3 = series.quantile(0.95)\n",
        "            if q1 == q3:\n",
        "                return pd.Series(0.5, index=series.index)  # Distribution too narrow\n",
        "            return (series - q1) / (q3 - q1)\n",
        "        else:\n",
        "            # Rolling robust scaling\n",
        "            def rolling_quantile(x, q):\n",
        "                return x.quantile(q)\n",
        "\n",
        "            q1s = series.rolling(window=window, min_periods=1).apply(\n",
        "                lambda x: rolling_quantile(x, 0.05), raw=False)\n",
        "            q3s = series.rolling(window=window, min_periods=1).apply(\n",
        "                lambda x: rolling_quantile(x, 0.95), raw=False)\n",
        "\n",
        "            # Handle case where q1 equals q3\n",
        "            denom = (q3s - q1s).replace(0, np.nan).fillna(1)\n",
        "            return (series - q1s) / denom\n",
        "\n",
        "    elif method.lower() == 'deviation_from_mean':\n",
        "        if window == len(series):\n",
        "            # Global deviation from mean\n",
        "            return series - series.mean()\n",
        "        else:\n",
        "            # Rolling deviation from mean\n",
        "            means = series.rolling(window=window, min_periods=1).mean()\n",
        "            return series - means\n",
        "\n",
        "    # Non-Stationary Time Series Normalization Methods\n",
        "    elif method.lower() == 'differencing':\n",
        "        # First-order differencing\n",
        "        return series.diff().fillna(0)\n",
        "\n",
        "    elif method.lower() == 'percentage_change':\n",
        "        # Percentage change\n",
        "        pct_change = series.pct_change().fillna(0) * 100\n",
        "        # Replace infinite values with large but finite numbers\n",
        "        pct_change = pct_change.replace([np.inf, -np.inf], [10000, -10000])\n",
        "        return pct_change\n",
        "\n",
        "    elif method.lower() == 'log_transform':\n",
        "        # Log transformation (with small constant to handle zeros/negatives)\n",
        "        min_val = min(series.min(), 0)\n",
        "        offset = 1 - min_val if min_val <= 0 else 0\n",
        "        return np.log(series + offset)\n",
        "\n",
        "    elif method.lower() == 'box_cox':\n",
        "        # Box-Cox transformation (auto-find lambda)\n",
        "        min_val = min(series.min(), 0)\n",
        "        offset = 1 - min_val if min_val <= 0 else 0\n",
        "        adjusted_series = series + offset\n",
        "\n",
        "        # Find optimal lambda between -2 and 2\n",
        "        lambda_seq = np.linspace(-2, 2, 100)\n",
        "        _, lambda_value = stats.boxcox(adjusted_series, lmbda=lambda_seq)\n",
        "\n",
        "        # Apply Box-Cox with found lambda\n",
        "        if abs(lambda_value) < 1e-10:  # Close to zero\n",
        "            return np.log(adjusted_series)\n",
        "        else:\n",
        "            return (adjusted_series ** lambda_value - 1) / lambda_value\n",
        "\n",
        "    elif method.lower() == 'detrending':\n",
        "        # Linear detrending\n",
        "        x = np.arange(len(series))\n",
        "        slope, intercept, _, _, _ = stats.linregress(x, series.values)\n",
        "        trend = pd.Series(intercept + slope * x, index=series.index)\n",
        "        return series - trend\n",
        "\n",
        "    elif method.lower() == 'rolling_normalization':\n",
        "        # Rolling window Z-score normalization\n",
        "        means = series.rolling(window=window, min_periods=1).mean()\n",
        "        stds = series.rolling(window=window, min_periods=1).std(ddof=0)\n",
        "        # Prevent division by zero\n",
        "        stds = stds.replace(0, np.nan).fillna(1)\n",
        "        return (series - means) / stds\n",
        "\n",
        "    # If method not recognized, return original series\n",
        "    return series\n",
        "\n",
        "def plot_signals(signals, raw_data=None, figsize=(12, 15), color_map='viridis', grid=True,\n",
        "               title='Signal Extraction Results', date_column=None):\n",
        "    \"\"\"\n",
        "    Plot extracted signals in multiple stacked charts.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    signals : pandas.DataFrame\n",
        "        DataFrame containing the extracted signals\n",
        "    raw_data : pandas.DataFrame, optional\n",
        "        Original data frame to optionally plot alongside signals\n",
        "    figsize : tuple, optional\n",
        "        Figure size (width, height) in inches\n",
        "    color_map : str, optional\n",
        "        Matplotlib colormap name to use for signal lines\n",
        "    grid : bool, optional\n",
        "        Whether to display grid lines\n",
        "    title : str, optional\n",
        "        Main title for the figure\n",
        "    date_column : str, optional\n",
        "        If provided, use this column from raw_data as x-axis (for time series data)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.figure.Figure\n",
        "        The created figure object\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.dates as mdates\n",
        "    from matplotlib.cm import get_cmap\n",
        "\n",
        "    # Get number of signals to plot\n",
        "    n_signals = len(signals.columns)\n",
        "\n",
        "    if n_signals == 0:\n",
        "        print(\"No signals to plot\")\n",
        "        return None\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, axes = plt.subplots(n_signals, 1, figsize=figsize, sharex=True)\n",
        "\n",
        "    # If only one signal, axes won't be an array\n",
        "    if n_signals == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Get colormap\n",
        "    cmap = get_cmap(color_map)\n",
        "    colors = [cmap(i/max(1, n_signals-1)) for i in range(n_signals)]\n",
        "\n",
        "    # Prepare x-axis data\n",
        "    if date_column is not None and raw_data is not None and date_column in raw_data.columns:\n",
        "        x_data = raw_data[date_column]\n",
        "        x_is_date = pd.api.types.is_datetime64_any_dtype(x_data)\n",
        "    else:\n",
        "        x_data = signals.index\n",
        "        x_is_date = pd.api.types.is_datetime64_any_dtype(x_data)\n",
        "\n",
        "    # Plot each signal\n",
        "    for i, col in enumerate(signals.columns):\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Plot the signal\n",
        "        ax.plot(x_data, signals[col], color=colors[i], linewidth=1.5, label=col)\n",
        "\n",
        "        # Add original data if available (as a light gray line)\n",
        "        if raw_data is not None and col in raw_data.columns:\n",
        "            ax.plot(x_data, raw_data[col], color='lightgray', linestyle='--', alpha=0.6,\n",
        "                   linewidth=1, label=f'Original {col}')\n",
        "\n",
        "        # Configure plot\n",
        "        ax.set_title(f'Signal: {col}', fontsize=10, pad=5)\n",
        "        ax.grid(grid, linestyle=':', alpha=0.6)\n",
        "        ax.legend(loc='upper right', fontsize=9)\n",
        "\n",
        "        # Format y-axis\n",
        "        formatter = plt.ScalarFormatter(useOffset=False)\n",
        "        formatter.set_scientific(False)\n",
        "        ax.yaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # Customize for time series data\n",
        "        if x_is_date:\n",
        "            locator = mdates.AutoDateLocator()\n",
        "            formatter = mdates.ConciseDateFormatter(locator)\n",
        "            ax.xaxis.set_major_locator(locator)\n",
        "            ax.xaxis.set_major_formatter(formatter)\n",
        "\n",
        "    # Set main title\n",
        "    fig.suptitle(title, fontsize=14, y=0.99)\n",
        "\n",
        "    # Adjust layout\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.95)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# Signal configuration\n",
        "signal_config = {\n",
        "    'price': ['column', 'ema', 10, 'z_score', 20],\n",
        "    'volume': ['column', 'sma', 5, 'min_max', 15],\n",
        "    'momentum': ['column', 'hma', 7, 'rolling_normalization', 10],\n",
        "    'volatility': ['column', 'kalman', 5, 'percentage_change', None]\n",
        "}\n",
        "\n",
        "# Extract signals\n",
        "signals = extract_signals(raw_data, signal_config)\n",
        "\n",
        "# Plot signals\n",
        "fig = plot_signals(signals, raw_data, figsize=(12, 10), date_column='date')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bN_81svot2Vc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}